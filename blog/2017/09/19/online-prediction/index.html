
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Online Prediction - Que-sais je?</title>
  <meta name="author" content="yhli">

  
  <meta name="description" content="Introduction Can one predict the future, without any objective assumption? Indeed, there exists a prediction strategy, which works without any &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yenhuanli.github.io/blog/2017/09/19/online-prediction">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Que-sais je?" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-77006714-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Que-sais je?</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
</ul>
  
  
  
  
  
  
  
  
  
  
    
      <form action="https://www.google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="yenhuanli.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title">Online Prediction</h1>
      
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-09-19T21:29:38+02:00'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>9:29 pm</span></time>
        
        
      </p>
    
  </header>


<div class="entry-content"><h3 id="introduction">Introduction</h3>

<p>Can one predict the future, <em>without any objective assumption</em>? 
Indeed, there exists a prediction strategy, which works without any objective assumption, and competes with (arguably) the best possible strategy asymptotically.</p>

<p>Consider the following protocol. 
For <script type="math/tex">t = 1, 2, \ldots</script>,</p>

<ol>
  <li>Reality announces <script type="math/tex">x_t \in \mathcal{X}</script>.</li>
  <li>Predictor announces <script type="math/tex">\gamma_t \in \Gamma</script>.</li>
  <li>Reality announces <script type="math/tex">y_t \in \mathcal{Y}</script>.</li>
</ol>

<p><strong>Exercise.</strong> Find some real-life examples of this protocol. Notice that $\mathcal{X}$ can be the space of <em>all possible histories</em>.</p>

<p>A <em>prediction strategy</em> is defined by a function <script type="math/tex">\psi: \mathcal{X} \to \Delta (\Gamma)</script>, where <script type="math/tex">\Delta ( \Gamma )</script> denotes the set of probabiliy measures on <script type="math/tex">\Gamma</script>.
Having observed $x_t$, Predictor chooses <script type="math/tex">\gamma_t \in \Gamma</script> randomly according to $\psi ( x_t )$.</p>

<p><strong>Definition.</strong> A prediction strategy is called continuous, if the corresponding function <script type="math/tex">\psi</script> is continuous on $\mathcal{X}$.</p>

<p>We measure the quality of a prediction strategy by a loss function <script type="math/tex">\lambda: \mathcal{X} \times \Gamma \times \mathcal{Y} \to \mathbb{R}</script>.</p>

<p><strong>Definition.</strong> A prediction strategy is called <em>universally consistent</em>, if whenever <script type="math/tex">\{ x_1, x_2, \ldots \}</script> and <script type="math/tex">\{ y_1, y_2, \ldots \}</script> are precompact, it holds that</p>

<script type="math/tex; mode=display">\limsup_{T \to \infty} \frac{1}{T} \sum_{t = 1}^T \left( \lambda ( x_t, \gamma_t, y_t ) - \lambda( x_t, g_t, y_t ) \right) \leq 0 , \quad \text{a.s.},</script>

<p>for any sequence <script type="math/tex">( g_t )_{t \in \mathbb{N}}</script> generated by a continuous prediction strategy.</p>

<p>That is, asymptotically, a universally consistent prediction strategy performs at least as well as the best continuous prediction strategy.</p>

<p><strong>Theorem 1.</strong> Suppose that <script type="math/tex">\mathcal{X}</script> and <script type="math/tex">\mathcal{Y}</script> are locally compact metric spaces, <script type="math/tex">\Gamma</script> is a metric space, and <script type="math/tex">\lambda</script> is a continuous compact-type function. Then there exists a universally consistent prediction strategy.</p>

<p>The proof of Theorem 1 is technical. 
For ease of illustration, only the following simplified version will be considered.</p>

<p><strong>Theorem 2.</strong> Suppose that <script type="math/tex">\mathcal{X}</script>, <script type="math/tex">\mathcal{Y}</script>, and  <script type="math/tex">\Gamma</script> are compact sets in Euclidean spaces, and <script type="math/tex">\lambda</script> is a continuous function. Then there exists a universally consistent prediction strategy.</p>

<p>This post is based on <a href="https://arxiv.org/abs/cs/0606093">an article by V. Vovk</a>.</p>

<h3 id="probability-forecasting">Probability Forecasting</h3>

<p>Intuitively, if we have access to the probability distribution of $y_t$ conditional on all past $x_t$’s and $g_t$’s, making predictions can be easier, as then the prediction protocol is just probabilistically random but not <em>uncertain</em>. 
While the protocol above is completely deterministic, it is still possible to do <em>probability forecasting</em>, to produce some estimate of the <em>conditional probability distribution</em>. 
We will see that such a good probability forecasting strategy permits one to construct a universally consistent prediction strategy.</p>

<p>The protocol for probability forecasting is as follows. 
For $t = 1, 2, \ldots$,</p>

<ol>
  <li>Reality announces <script type="math/tex">x_t \in \mathcal{X}</script>.</li>
  <li>Forecaster announces <script type="math/tex">P_t \in \Delta ( \mathcal{Y} )</script>.</li>
  <li>Reality announces $y_t \in \mathcal{Y}$.</li>
</ol>

<p><strong>Exercise.</strong> Find some real-life examples of the protocol.
A famous one is weather forecasting; see, e.g., <a href="http://www.jstor.org/stable/2287720">this paper by A. P. Dawid</a>.</p>

<p><strong>Definition.</strong> We say that a forecasting strategy is calibrated, if</p>

<script type="math/tex; mode=display">\lim_{T \to \infty} \frac{1}{T} \sum_{t = 1}^T \left( f ( x_t, P_t, y_t ) - \int_{\mathcal{Y}} f ( x_t, P_t, y ) P_t ( \mathrm{d} y ) \right) = 0 ,</script>

<p>for all continuous functions $f: \mathcal{X} \times \Delta ( \mathcal{Y} ) \times \mathcal{Y} \to \mathbb{R}$.</p>

<p><strong>Remark.</strong> This is not any of the standard definitions of calibration. 
However, one can observe that it is closely related to the notion of <em>weak calibration</em> <a href="https://doi.org/10.1016/j.jcss.2007.04.017">proposed by S. M. Kakade and D. P. Foster</a>, and the notion of <em>calibration-cum-resolution</em> <a href="https://doi.org/10.1016/j.tcs.2007.07.026">proposed by V. Vovk</a>.</p>

<p>One may view the definition above as specifying certain law of large numbers.</p>

<p><strong>Theorem 3. (<a href="https://arxiv.org/abs/cs/0606093">Vovk</a>)</strong> Suppose that $\mathcal{X}$ and $\mathcal{Y}$ are compact sets in Euclidean spaces. 
Then there exists a calibrated forecasting strategy.</p>

<h3 id="proof-of-theorem-3">Proof of Theorem 3</h3>

<p>Theorem 3 relies on the fact that for any compact metric space $\Omega$, there always exists some reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ on it, such that the corresponding kernel function $k: \Omega \times \Omega \to \mathbb{R}$ is continuous, the RKHS is dense in $C( \Omega )$, and the <em>embedding constant</em>, defined as</p>

<script type="math/tex; mode=display">c_{\mathcal{H}} := \sup_{\omega \in \Omega} \sqrt{ k ( \omega, \omega ) } ,</script>

<p>is finite. 
We call such an RKHS a continuous universal RKHS with finite embedding constant on $\Omega$. 
See Corollary 1 in <a href="https://arxiv.org/abs/cs/0606093">the article by V. Vovk</a>.</p>

<p>Following the idea of <em>defensive forecasting</em>, consider the following protocol. 
Let $C_0$ be a positive number. 
For $t = 1, 2, \ldots$,</p>

<ol>
  <li>Reality announces $x_t \in \mathcal{X}$.</li>
  <li>Skeptic announces a continuous function $f_t: \mathcal{Y} \times \Delta ( \mathcal{Y} ) \to \mathbb{R}$, such that $\int_{\mathcal{Y}} f_t ( y, P ) P ( \mathrm{d} y ) \leq 0$ for all $P \in \Delta ( \mathcal{Y} )$.</li>
  <li>Forecaster announces $P_t \in \Delta( \mathcal{Y} )$.</li>
  <li>Reality announces $y_t \in \mathcal{Y}$.</li>
  <li>$C_t := C_{t - 1} + f_t ( y_t, P_t )$.</li>
</ol>

<p>The following lemma, arguably the key to the idea of defensive forecasting, can be checked by Ky Fan’s minimax theorem.</p>

<p><strong>Lemma 1.</strong> Let $\mathcal{Y}$ be a metric compact. 
Forecaster can force the sequence $( C_t )_{t = 0, 1, \ldots}$ to be non-increasing, by choosing</p>

<script type="math/tex; mode=display">P_t \in \mathrm{argmin}_{P \in \Delta ( \mathcal{Y} )} \mathrm{sup}_{Q \in \Delta ( \mathcal{Y} )} \int_{\mathcal{Y}} f_t (  y, P) Q( \mathrm{d} y ) .</script>

<p>In the defensive forecasting protocol, we choose a specific $f_t$ as follows. 
Let $\mathcal{H}$ be a continuous universal RKHS with finite embedding constant on $\mathcal{X} \times \Delta ( \mathcal{Y}  ) \times \mathcal{Y}$, and let $k_{x, P, y} \in \mathcal{H}$ be the corresponding representer (i.e., $\left\langle k_{x, P, y}, f \right\rangle_{\mathcal{H}} = f ( x, P, y )$ for all $f \in \mathcal{H}$).
Set</p>

<script type="math/tex; mode=display">f_t ( y, P ) := 2 \left\langle \sum_{\tau = 1}^{t - 1} \Psi ( x_\tau, P_\tau, y_\tau ), \Psi( x_t, P, y ) \right\rangle_{\mathcal{H}},</script>

<p>where</p>

<script type="math/tex; mode=display">\Psi ( x, P, y ) := k_{x, P, y} - \int_{\mathcal{Y}} k_{x, P, y} P ( \mathrm{d} y ) .</script>

<p>The following lemma is an application of Lemma 1 (with non-trivial derivations!).</p>

<p><strong>Lemma 2.</strong> It holds that, for all $T$ and $f \in \mathcal{H}$,</p>

<script type="math/tex; mode=display">\left\vert \sum_{t = 1}^T \left( f ( x_t, P_t, y_t ) - \int_{\mathcal{Y}} f ( x_t, P_t, y ) P_t ( \mathrm{d} y ) \right) \right\vert \leq 2 c_{\mathcal{H}} \left\Vert f \right\Vert_{\mathcal{H}} \sqrt{T} .</script>

<p><em>Proof of Theorem 3.</em></p>
</div>


  <footer>
    <p class="meta">
      
  



  <span class="byline author vcard">Authored by <span class="fn">
  
    yhli
  
  </span></span>


      




<time class='entry-date' datetime='2017-09-19T21:29:38+02:00'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>9:29 pm</span></time>
      
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://yenhuanli.github.io/blog/2017/09/19/online-prediction/" data-via="" data-counturl="http://yenhuanli.github.io/blog/2017/09/19/online-prediction/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/05/05/mirror-descent-str/" title="Previous Post: Minimizing a Strongly Convex Function by Mirror Descent">&laquo; Minimizing a Strongly Convex Function by Mirror Descent</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/09/20/universally-consistent-prediction/" title="Next Post: Universally Consistent Prediction">Universally Consistent Prediction &raquo;</a>
      
    </p>
  </footer>
</article>


</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - yhli -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
