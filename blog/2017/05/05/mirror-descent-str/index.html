
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Minimizing a Strongly Convex Function by Mirror Descent - Que-sais je?</title>
  <meta name="author" content="yhli">

  
  <meta name="description" content="Mirror descent (MD) is a famous convex optimization algorithm. When the objective function is Lipschitz continuous, the convergence rate of MD is &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yenhuanli.github.io/blog/2017/05/05/mirror-descent-str">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Que-sais je?" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-77006714-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Que-sais je?</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
</ul>
  
  
  
  
  
  
  
  
  
  
    
      <form action="https://www.google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="yenhuanli.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title">Minimizing a Strongly Convex Function by Mirror Descent</h1>
      
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-05-05T15:45:42+02:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>3:45 pm</span></time>
        
        
      </p>
    
  </header>


<div class="entry-content"><p>Mirror descent (MD) is a famous convex optimization algorithm. 
When the objective function is Lipschitz continuous, the convergence rate of MD is known to be $O ( t^{-1/2} )$. 
When the objective function is also strongly convex, intuitively, a better convergence rate should be possible.
Suprisingly, there are very few related results in existing literature, to the best of my knowledge. 
This post summarizes one such result in a <a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">review article by Juditsky and Nemirovski</a>.</p>

<p>We will consider a standard constrained convex optimization problem:</p>

<script type="math/tex; mode=display">f^\star = \mathrm{arg\, min} \left\lbrace f ( x ) | x \in \mathcal{X} \right\rbrace ,</script>

<p>where $f$ is a convex function, and $\mathcal{X}$ is a compact convex set in $\mathbb{R}^d$. 
We assume that $f$ is $L$-Lipschitz continuous w.r.t. some norm $\Vert \cdot \Vert$ on $\mathbb{R}^d$.</p>

<h3 id="brief-review-of-mirror-descent">Brief Review of Mirror Descent</h3>

<p>Let $\omega$ be a continuously differentiable $1$-strongly convex function on $\mathcal{X}$, w.r.t. $\Vert \cdot \Vert$.
Define the corresponding <em>Bregman divergence</em></p>

<script type="math/tex; mode=display">D ( y, x ) := \omega ( y ) - \left[ \omega ( x ) + \left\langle \omega' ( x ), y - x \right\rangle \right] ,</script>

<p>and <em>prox-mapping</em></p>

<script type="math/tex; mode=display">\mathrm{prox} ( \xi; x ) := \mathrm{arg\, min} \left\lbrace \left\langle \xi, u \right\rangle + D ( u, x ) \, \middle| \, x \in \mathcal{X} \right\rbrace .</script>

<p>Let $x_1 \in \mathcal{X}$, and $( \eta_1, \eta_2, \ldots )$ be a sequence of <em>step sizes</em>. 
The <em>standard</em> MD iterates as follows.</p>

<script type="math/tex; mode=display">x_{t + 1} = \mathrm{prox} ( \eta_t f' ( x_t ) ; x_t ) , \quad t = 1, 2, \ldots .</script>

<p>Define</p>

<script type="math/tex; mode=display">x^t := \frac{ \sum_{\tau = 1}^t \eta_\tau x_\tau }{ \sum_{\tau = 1}^t \eta_\tau } .</script>

<p>The following result can be found in, e.g., <a href="https://web.iem.technion.ac.il/images/user-files/becka/papers/3.pdf">the classic paper by Beck and Teboulle</a>.</p>

<p><strong>Proposition 1.</strong> It holds that, for any $u \in \mathcal{X}$,</p>

<script type="math/tex; mode=display">f ( x^t ) - f ( u ) \leq \frac{ D ( u, x_1 ) + \frac{1}{2} \sum_{\tau = 1}^t \eta_\tau^2 \left\Vert f' ( x_\tau ) \right\Vert_*^2 }{ \sum_{\tau = 1}^t \eta_\tau } ,</script>

<p>where $\left\Vert \cdot \right\Vert_*$ denotes the dual norm.</p>

<p>Proposition 1 implies the following convergence guarantee for the standard MD.</p>

<p><strong>Corollary.</strong> Fix a positive integer $T$.
Set</p>

<script type="math/tex; mode=display">\Omega \geq \max \left\lbrace D ( u, x_1 ) \, \middle| \, u \in \mathcal{X} \right\rbrace , \quad \eta_t = \frac{\sqrt{2 \Omega }}{ L \sqrt{T}} \,\, \text{ for } t = 1, 2, \ldots, T .</script>

<p>Then</p>

<script type="math/tex; mode=display">f ( x^T ) - f^\star \leq \frac{L \sqrt{2 \Omega}}{\sqrt{T}} .</script>

<h3 id="a-modified-md-for-minimizing-strongly-convex-functions">A Modified MD for Minimizing Strongly Convex Functions</h3>

<p>Now assume that $f$ is also $\mu$-strongly convex w.r.t. $\Vert \cdot \Vert$.
How do we exploit this additional information?</p>

<p>Let us choose $\omega$ such that it is $1$-strongly convex on the whole $\mathbb{R}^d$, <em>instead of merely $\mathcal{X}$</em>. 
For any $R &gt; 0$, define $\omega_{R, z} ( x ) := \omega ( R^{-1} ( x - z ) )$; denote by $D_{R, z}$ the corresponding Bregman divergence, and $\mathrm{prox}_{R, z}$ the corresponding prox-mapping.</p>

<p>Let us define an iteration rule very similar to the standard MD; the only difference is that now we replace $\omega$ by $\omega_{R, z}$ for some $z \in \mathbb{R}^d$: 
Let $( \eta_1, \eta_2, \ldots )$ be a given sequence of step sizes.
For any $x_1 \in \mathcal{X}$, we compute</p>

<script type="math/tex; mode=display">x_{t + 1} = \mathrm{prox}_{R, z} ( \eta_t f' ( x_t ) ; x_t ) , \quad t = 1, 2, \ldots .</script>

<p>Define</p>

<script type="math/tex; mode=display">x^t ( R, z, x_1 ) := \frac{ \sum_{\tau = 1}^t \eta_\tau x_\tau }{ \sum_{\tau = 1}^t \eta_\tau } .</script>

<p><strong>Proposition 2.</strong> Fix a positive integer $T$.
Set</p>

<script type="math/tex; mode=display">\Omega \geq \max \left\lbrace D ( u, x_1 ) \, \middle| \, \left\Vert u \right\Vert \leq 1, u \in \mathbb{R}^d \right\rbrace , \quad R_0 \geq \left\Vert x_1 - x^\star \right\Vert , \quad \eta_t = \frac{\sqrt{2 \Omega }}{ R_0 L \sqrt{T}} \,\, \text{ for all } t .</script>

<p>Then</p>

<script type="math/tex; mode=display">f ( x^T ( R_0, x_1, x_1 ) ) - f^\star \leq \frac{R_0 L \sqrt{2 \Omega}}{\sqrt{T}}, \quad \left\Vert x^T ( R_0, x_1, x_1 ) - x^\star \right\Vert^2 \leq \frac{R_0 L \sqrt{8 \Omega}}{\mu \sqrt{T}} ,</script>

<p>where $x^\star$ is the unique minimizer of $f$ on $\mathcal{X}$.</p>

<p><em>Proof.</em>
Notice that $\omega_R$ is $1$-strongly convex w.r.t. $\Vert \cdot \Vert_R := R^{-1} \Vert \cdot \Vert$. 
The first inequality follows from Proposition 1, using the norm $\Vert \cdot \Vert_R$. 
The second inequality follows from the following two inequalities, obtained by the strong convexity of $f$ and the optimality of $x^\star$, respectively:</p>

<script type="math/tex; mode=display">f ( x^T ) - f^\star \geq \left\langle f' ( x^\star ) , x^T - x^\star \right\rangle + \frac{\mu}{2} \Vert x^T - x^\star \Vert^2, \quad \left\langle f' ( x^\star ), x^T - x^\star \right\rangle \geq 0 .</script>

<p><em>Q.E.D.</em></p>

<p>Now we have an error bound depending on $R_0$; the smaller $R_0$ is, the smaller the error bounds are.
Also notice that the bound of the distance between $x^T$ and $x^\star$ is strictly decreasing with $T$. 
These observations motivate the following <em>restarting</em> strategy: 
Set $y_0 \in \mathcal{X}$. 
For $k = 1, 2, \ldots$,</p>

<ol>
  <li>Set $T_k$ such that $\left\Vert x^{T_k} ( R_{k - 1}, y_{k - 1}, y_{k - 1} ) - x^\star \right\Vert^2 \leq 2^{-1} R_{k - 1}^2$.</li>
  <li>Compute $y_k = x^{T_k} ( R_{k - 1}, y_{k -1}, y_{k - 1} )$, with $\eta_t = \frac{\sqrt{2 \Omega }}{ R L \sqrt{T_k}}$ for all $t$.</li>
  <li>Set $R_k^2 = 2^{-1} R_{k - 1}^2$.</li>
</ol>

<p>By the proposition we have just proved, it suffices to choose</p>

<script type="math/tex; mode=display">T_k = \left\lceil \frac{32 L^2 \Omega}{\mu^2 R_{k - 1}^2} \right\rceil .</script>

<p><strong>Proposition 3.</strong> Define $M_k = \sum_{\kappa = 1}^k T_{\kappa}$. 
Let $k^\star$ be the smallest $k$ such that</p>

<script type="math/tex; mode=display">k \leq \frac{L^2 \Omega}{\mu^2 R_0^2} 2^{k + 5} .</script>

<p>Then for $k &lt; k^\star$,</p>

<script type="math/tex; mode=display">f ( y_k ) - f^\star \leq 2^{-(0.5 M_k + 1)} \mu R_0^2, \quad \left\Vert y_k - x^\star \right\Vert^2 \leq 2^{- 0.5 M_k} R_0^2 ;</script>

<p>for $k \geq k^\star$,</p>

<script type="math/tex; mode=display">f ( y_k ) - f^\star \leq \frac{32 L^2 \Omega}{\mu M_k}, \quad \left\Vert y_k - x^\star \right\Vert^2 \leq \frac{64 L^2 \Omega}{\mu^2 M_k} .</script>

<p><em>Proof.</em> It is easily checked, by Proposition 2, that</p>

<script type="math/tex; mode=display">f ( y_k ) - f^\star \leq 2^{-(k + 1)} \mu R_0^2, \quad \left\Vert y_k - x^\star \right\Vert^2 \leq 2^{-k} R_0^2 .</script>

<p>By our choice of $T_k$, it holds that</p>

<script type="math/tex; mode=display">M_k \leq k + \sum_{\kappa = 1}^k \frac{32 L^2 \Omega}{\mu^2 R_{\kappa - 1}^2} = k + \sum_{\kappa = 1}^k \frac{32 L^2 \Omega}{\mu^2 2^{-(\kappa - 1)} R_0^2} \leq k + \frac{L^2 \Omega}{\mu^2 R_0^2} 2^{k + 5} .</script>

<p>Therefore, $M_k \leq 2 k$ for $k &lt; k^\star$, and</p>

<script type="math/tex; mode=display">M_k \leq \frac{L^2 \Omega}{\mu^2 R_0^2} 2^{k + 6}</script>

<p>for $k \geq k^\star$. 
<em>Q.E.D.</em></p>

<p>Notice that to compute each $y^k$, we need to compute $T_k$ additional prox-mappings.
Therefore, the effective number of iterations is represented by $M_k$, instead of $k$, and the convergence rate is understood as</p>

<script type="math/tex; mode=display">\text{gap to the optimal value} = O ( 1 / ( \text{effective number of iterations} ) ) .</script>
</div>


  <footer>
    <p class="meta">
      
  



  <span class="byline author vcard">Authored by <span class="fn">
  
    yhli
  
  </span></span>


      




<time class='entry-date' datetime='2017-05-05T15:45:42+02:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>3:45 pm</span></time>
      
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://yenhuanli.github.io/blog/2017/05/05/mirror-descent-str/" data-via="" data-counturl="http://yenhuanli.github.io/blog/2017/05/05/mirror-descent-str/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/04/12/super-martingale/" title="Previous Post: Defensive Forecasting">&laquo; Defensive Forecasting</a>
      
      
    </p>
  </footer>
</article>


</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - yhli -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
