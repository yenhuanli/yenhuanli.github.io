
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Minimizing Strongly Convex Functions by Mirror Descent - Que-sais je?</title>
  <meta name="author" content="yhli">

  
  <meta name="description" content="Mirror descent is an algorithm to numerically solve convex optimization problems, very popular in machine learning.
Surprisingly, there are very few &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yenhuanli.github.io/blog/2017/05/01/mirror-descent">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Que-sais je?" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-77006714-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Que-sais je?</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
</ul>
  
  
  
  
  
  
  
  
  
  
    
      <form action="https://www.google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="yenhuanli.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title">Minimizing Strongly Convex Functions by Mirror Descent</h1>
      
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-05-01T14:38:13+02:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>1</span><span class='date-suffix'>st</span>, <span class='date-year'>2017</span></span> <span class='time'>2:38 pm</span></time>
        
        
      </p>
    
  </header>


<div class="entry-content"><p>Mirror descent is an algorithm to numerically solve convex optimization problems, very popular in machine learning.
Surprisingly, there are very few existing results on mirror descent applied to strongly convex objective functions. 
This post summarizes one such result, which I found in <a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">a review paper by Juditsky and Nemirovski</a>.</p>

<h3 id="mirror-descent">Mirror Descent</h3>

<p>Consider the convex optimization problem</p>

<script type="math/tex; mode=display">f^\star = \min \lbrace f ( x ) | x \in \mathcal{X} \rbrace,</script>

<p>where $f$ is a real-valued Lipschitz continuous convex function, and $\mathcal{X}$ is a closed convex set in $\mathbb{R}^d$.</p>

<p>To define mirror descent, we need the notion of a <em>distance generating function (DGF)</em>.
Fix a norm $\Vert \cdot \Vert$ on $\mathbb{R}^d$. 
A DGF $\omega$ is a real-valued convex function continuously differentiable on $\mathcal{X}$, such that for all $x, y \in \mathcal{X}$,</p>

<script type="math/tex; mode=display">\omega ( y ) \geq \omega ( x ) + \langle \omega' ( x ), y - x \rangle + \frac{1}{2} \Vert y - x \Vert^2 .</script>

<p>Fix a DGF $\omega$.
The corresponding <em>Bregman divergence</em> is given by</p>

<script type="math/tex; mode=display">D_\omega ( y, x ) := \omega ( y ) - \omega ( x ) - \langle \omega' ( x ) , y - x \rangle .</script>

<p>The corresponding <em>prox-mapping</em> is given by</p>

<script type="math/tex; mode=display">\mathrm{prox}_\omega ( \xi; x ) := \mathrm{arg\, max} \lbrace \langle \xi, u \rangle + D_\omega ( u, x ) \vert u \in \mathcal{X} \rbrace .</script>

<p>Mirror descent iterates as follows.</p>

<ul>
  <li>Choose $x_1 \in \mathcal{X}$.</li>
  <li>Let $( \eta_1, \eta_2, \ldots )$ be a given sequence of positive numbers.</li>
  <li>For $t = 1, 2, \ldots$, compute</li>
</ul>

<script type="math/tex; mode=display">x_{t+1} = \mathrm{prox}_\omega ( \eta_t f' ( x_t ) ; x_{t} ) .</script>

<ul>
  <li>Set $x^t$ to be the weighted average of $x_1, \ldots, x_t$ as</li>
</ul>

<script type="math/tex; mode=display">x^t := \frac{\sum_{\tau = 1}^t \eta_\tau x_\tau}{\sum_{\tau = 1}^t \eta_\tau} .</script>

<p>Notice that $f$ is not necessarily differentiable; $f’ (x)$ is understood as a subgradient of $f$ at $x$.</p>

<p><strong>Example.</strong> The following might be the most well-known example.
Suppose that $\mathcal{X}$ is the probability simplex in $\mathbb{R}^d$. 
Choose $\omega ( x ) = \sum_{i = 1}^d x_i \log x_i - \sum_{i = 1}^d x_i$, which, by Pinsker’s inequality, is strongly convex with respect to the $\ell_1$-norm.
Then $x_{t + 1}$ has a closed-form:</p>

<script type="math/tex; mode=display">( x_{t + 1} )_i = \frac{ ( x_t )_i \mathrm{e}^{- \eta_t ( f' ( x_t ) )_i} }{ \sum_{j = 1}^d ( x_t )_j \mathrm{e}^{- \eta_t ( f' ( x_t ) )_j} } ,  \quad i = 1, 2, \ldots, d ,</script>

<p>where $( v )_i$ denotes the $i$-th element of any vector $v$.</p>

<p><strong>Example.</strong> Notice that choosing $\omega$ to be the $\ell_2$-norm is always valid. 
In this case, we recover the <a href="https://en.wikipedia.org/wiki/Subgradient_method#Projected_subgradient">projected subgradient method</a>.</p>

<h3 id="minimizing-lipschitz-continuous-functions">Minimizing Lipschitz Continuous Functions</h3>

<p>It is well-known that mirror descent achieves $O ( 1 / \sqrt{k} )$ convergence rate, as long as $f$ is Lipschitz continuous (and, of course, convex).
Below is the proof given in <a href="https://doi.org/10.1016/S0167-6377(02)00231-6">a classic paper by Beck and Teboulle</a>.</p>

<p><strong>Proposition.</strong> For any $u \in \mathcal{X}$,</p>

<script type="math/tex; mode=display">f ( x^t ) - f ( u ) \leq \frac{ D ( u, x_1 ) + \frac{1}{2} \sum_{\tau = 1}^t \eta_\tau^2 \Vert f' ( x_\tau ) \Vert_*^2 }{ \sum_{\tau = 1}^t \eta_\tau } .</script>

<p><em>Proof.</em> By convexity of $f$, we have</p>

<script type="math/tex; mode=display">f ( x^t ) - f ( u ) \leq \frac{1}{ \sum_{\tau = 1}^t \eta_\tau } \sum_{\tau = 1}^t \eta_\tau \left( f ( x_\tau ) - f ( u ) \right) \leq \frac{1}{ \sum_{\tau = 1}^t \eta_\tau } \sum_{\tau = 1}^t \eta_\tau \langle f' ( x_\tau ) , x_\tau - u \rangle .</script>

<p>Now we derive an upper bound of $\eta_\tau \langle f’ ( x_\tau ), x_\tau - u \rangle$.
The optimality condition says</p>

<script type="math/tex; mode=display">\langle \eta_\tau f' ( x_\tau ) + \omega'( x_{\tau + 1} ) - \omega' ( x_\tau ) , u - x_{\tau + 1} \rangle \geq 0 ,</script>

<p>from which we obtain</p>

<script type="math/tex; mode=display">\eta_\tau \langle f' ( x_\tau ), x_\tau - u \rangle \leq \eta_\tau \langle f' ( x_\tau ), x_\tau - x_{\tau + 1} \rangle + \langle \omega' ( x_{\tau + 1} ) - \omega' ( x_\tau ) , u - x_{\tau + 1} \rangle .</script>

<p>Applying the <em>three-point equality</em> (which is easy to check by direct calculation), we obtain</p>

<script type="math/tex; mode=display">\eta_\tau \langle f' ( x_\tau ), x_\tau - u \rangle \leq \eta_\tau \langle f' ( x_\tau ), x_\tau - x_{\tau + 1} \rangle + D_\omega ( u, x_\tau ) - D_\omega ( u, x_{\tau + 1} ) - D_\omega ( x_{\tau + 1}, x_\tau ) .</script>

<p>By the strong convexity of $\omega$, we write</p>

<script type="math/tex; mode=display">\eta_\tau \langle f' ( x_\tau ), x_\tau - x_{\tau + 1} \rangle - D_\omega ( x_{\tau + 1}, x_\tau ) \leq \eta_\tau \Vert f' ( x_\tau ) \Vert_* \Vert x_\tau - x_{\tau + 1} \Vert - \frac{1}{2} \Vert x_\tau - x_{\tau + 1} \Vert^2 \leq \frac{\eta_\tau^2}{2} \Vert f' ( x_\tau ) \Vert_*^2 ,</script>

<p>where $\Vert \cdot \Vert_*$ denotes the norm dual to $\Vert \cdot \Vert$.
Now we have</p>

<script type="math/tex; mode=display">\eta_\tau \langle f' ( x_\tau ), x_\tau - u \rangle \leq D_\omega ( u, x_\tau ) - D_\omega ( u, x_{\tau + 1} ) + \frac{\eta_\tau^2}{2} \Vert f' ( x_\tau ) \Vert_*^2 .</script>

<p>Summing over all $\tau$, the proposition follows. 
<em>Q.E.D.</em></p>

<p>Notice that $\Vert f’ ( x_t ) \Vert_* \leq L$ for all $t$.
The following corollary is obvious.</p>

<p><strong>Corollary.</strong> Set</p>

<script type="math/tex; mode=display">\eta_t = \frac{\eta}{\Vert f' ( x_t ) \Vert_* \sqrt{t}} ,</script>

<p>for some $\eta &gt; 0$. Then it holds that, for any $u \in \mathcal{X}$,</p>

<script type="math/tex; mode=display">f ( x^t ) - f ( u ) \leq \frac{L}{2 \sqrt{t}} \left[ \frac{D ( u, x_1 )}{\eta} + \frac{\eta}{2} \left( \log ( t ) + 1 \right) \right] .</script>

<p>The following corollary is technically more involved, and can be found in, e.g., <a href="https://doi.org/10.1016/S0167-6377(02)00231-6">the paper by Beck and Teboulle</a>.</p>

<p><strong>Corollary.</strong> Suppose that</p>

<script type="math/tex; mode=display">\Omega := \max \lbrace D_\omega ( u, x_1 ) | u \in \mathcal{X} \rbrace</script>

<p>is finite.
Set</p>

<script type="math/tex; mode=display">\eta_t = \frac{\sqrt{2 \Omega}}{ L \sqrt{t} } .</script>

<p>Then it holds that, for any $u \in \mathcal{X}$,</p>

<script type="math/tex; mode=display">f ( x^t ) - f ( u ) \leq L \sqrt{ \frac{2 \Omega}{t} } .</script>

<h3 id="minimizing-lipschitz-continuous-and-strongly-convex-functions">Minimizing Lipschitz Continuous and Strongly Convex Functions</h3>

<p>When the objective function is not only Lipschitz continuous but also strongly convex, it is reasonable to expect a convergence rate better than $O ( 1 / \sqrt{t} )$. 
The proof below, given in <a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">a review paper by Juditsky and Nemirovski</a>, shows that then $O ( 1 / t )$ can be achieved.</p>

<p>Let $\omega$ be a DGF for $\mathbb{R}^d$ (instead of merely $\mathcal{X}$!), with respect to a norm $\Vert \cdot \Vert$.
For any $R &gt; 0$, define $\omega_R ( \cdot ) := \omega ( R^{-1} \cdot )$; then $\omega_R$ is $1$-strongly convex with respect to $\Vert \cdot \Vert_R := R^{-1} \Vert \cdot \Vert$.
The corresponding Bregman divergence is given by</p>

<script type="math/tex; mode=display">D_{\omega, R} ( y, x ) := \omega_R ( y ) - \omega_R ( x ) - \langle \omega_R' ( x ), y - x \rangle.</script>

<p>The prox-mapping is given by</p>

<script type="math/tex; mode=display">\mathrm{prox}_{\omega, R} ( \xi; x ) := \mathrm{arg\, min} \lbrace \langle \xi, u \rangle + D_{\omega, R} ( u, x ) | u \in \mathcal{X} \rbrace .</script>

<p>Fix $x_1 \in \mathcal{X}$.
For any given sequence $(\eta_1, \eta_2, \ldots)$, define</p>

<script type="math/tex; mode=display">x_{t + 1} = \mathrm{prox}_{\omega, R} ( \eta_t f' ( x_t ); x_t ), \quad t = 1, 2, \ldots ,</script>

<p>and</p>

<script type="math/tex; mode=display">x^t ( R, x_1 ) := \frac{\sum_{\tau = 1}^t \eta_\tau x_\tau}{\sum_{\tau = 1}^t \eta_\tau} .</script>

<p><strong>Proposition.</strong> Suppose that $f$ is $L$-Lipschitz continuous and $\mu$-strongly convex on $\mathcal{X}$.
Suppose that</p>

<script type="math/tex; mode=display">\Omega := \max \lbrace D_\omega ( u, x_1 ) | u \in \mathcal{X} \rbrace</script>

<p>is finite.
Set $R$ such that $R \geq \Vert x_1 - x^\star \Vert$, where $x^\star$ denotes the minimizer of $f$ on $\mathcal{X}$. 
Set</p>

<script type="math/tex; mode=display">\eta_\tau = \frac{\sqrt{2 \Omega}}{R L \sqrt{t}}, \quad \tau = 1, 2, \ldots, t .</script>

<p>Then it holds that</p>

<script type="math/tex; mode=display">f ( x^t (R, x_1) ) - f ( x^\star ) \leq \frac{R L \sqrt{2 \Omega}}{\sqrt{t}}, \quad \Vert x^t (R, x_1) - x^\star \Vert^2 \leq \frac{2 R L \sqrt{ 2 \Omega }}{\mu \sqrt{t}} .</script>

<p><em>Proof.</em> Following the proof in the previous sectioin (with the norm $\Vert \cdot \Vert_R$), we have</p>

<script type="math/tex; mode=display">f ( x^t (R, x_1) ) - f ( x^\star ) \leq \frac{ \Omega + \frac{R^2 L^2}{2} \sum_{\tau = 1}^t \eta_\tau^2 }{ \sum_{\tau = 1}^t \eta_\tau } ,</script>

<p>which implies the first inequality with the specific choice of $( \eta_1, \eta_2, \ldots, \eta_t )$.
The second inequality follows from the strong convexity of $f$.
<em>Q.E.D.</em></p>

<p>Notice that the convergence speed depends on $R$, the initial distance between $x_1$ and $x^\star$.
When the current iterate is close enough to $x^\star$, which is guaranteed to happen with a large enough $t$, we can <em>restart</em> with a smaller value of $R$, in order to get a better convergence speed.</p>

<p>Consider the following algorithm.
Choose $y_0 \in \mathcal{X}$.
Set $R_0$ such that $R_0 \geq \Vert y_0 - x^\star \Vert$.
For $k = 1, 2, \ldots$,</p>

<ol>
  <li>Set $N_k = \left\lceil \frac{ 8 L^2 \Omega}{\mu^2 R_{k - 1}^2} \right\rceil$.</li>
  <li>Compute $y_k = x^{N_k} ( R_{k - 1}, y_{k - 1} )$, with $\eta_\tau = \frac{\sqrt{2 \Omega}}{R_{k - 1} L \sqrt{N_k}}$ for $\tau = 1, 2, \ldots, N_k$.</li>
  <li>Set $R_k^2 = 2^{-1} R_{k - 1}^2$.</li>
</ol>
</div>


  <footer>
    <p class="meta">
      
  



  <span class="byline author vcard">Authored by <span class="fn">
  
    yhli
  
  </span></span>


      




<time class='entry-date' datetime='2017-05-01T14:38:13+02:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>1</span><span class='date-suffix'>st</span>, <span class='date-year'>2017</span></span> <span class='time'>2:38 pm</span></time>
      
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://yenhuanli.github.io/blog/2017/05/01/mirror-descent/" data-via="" data-counturl="http://yenhuanli.github.io/blog/2017/05/01/mirror-descent/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/04/12/super-martingale/" title="Previous Post: Defensive Forecasting">&laquo; Defensive Forecasting</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/05/05/mirror-descent-str/" title="Next Post: Minimizing a Strongly Convex Function by Mirror Descent">Minimizing a Strongly Convex Function by Mirror Descent &raquo;</a>
      
    </p>
  </footer>
</article>


</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - yhli -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
