---
layout: post
title: "Online Prediction"
date: 2017-09-19 21:29:38 +0200
comments: true
categories: 
---

### Introduction

Can one predict the future, *without any objective assumption*? 
Indeed, there exists a prediction strategy, which works without any objective assumption, and competes with (arguably) the best possible strategy asymptotically. 

Consider the following protocol. 
For $$t = 1, 2, \ldots$$, 

1. Reality announces $$x_t \in \mathcal{X}$$. 
2. Predictor announces $$\gamma_t \in \Gamma$$. 
3. Reality announces $$y_t \in \mathcal{Y}$$.

**Exercise.** Find some real-life examples of this protocol. Notice that $\mathcal{X}$ can be the space of *all possible histories*.

A *prediction strategy* is defined by a function $$\psi: \mathcal{X} \to \Delta (\Gamma)$$, where $$\Delta ( \Gamma )$$ denotes the set of probabiliy measures on $$\Gamma$$.
Having observed $x_t$, Predictor chooses $$\gamma_t \in \Gamma$$ randomly according to $\psi ( x_t )$.

**Definition.** A prediction strategy is called continuous, if the corresponding function $$\psi$$ is continuous on $\mathcal{X}$.

We measure the quality of a prediction strategy by a loss function $$\lambda: \mathcal{X} \times \Gamma \times \mathcal{Y} \to \mathbb{R}$$. 

**Definition.** A prediction strategy is called *universally consistent*, if whenever $$\{ x_1, x_2, \ldots \}$$ and $$\{ y_1, y_2, \ldots \}$$ are precompact, it holds that

$$
\limsup_{T \to \infty} \frac{1}{T} \sum_{t = 1}^T \left( \lambda ( x_t, \gamma_t, y_t ) - \lambda( x_t, g_t, y_t ) \right) \leq 0 , \quad \text{a.s.}, 
$$

for any sequence $$( g_t )_{t \in \mathbb{N}}$$ generated by a continuous prediction strategy.

That is, asymptotically, a universally consistent prediction strategy performs at least as well as the best continuous prediction strategy. 

**Theorem 1.** Suppose that $$\mathcal{X}$$ and $$\mathcal{Y}$$ are locally compact metric spaces, $$\Gamma$$ is a metric space, and $$\lambda$$ is a continuous compact-type function. Then there exists a universally consistent prediction strategy. 

The proof of Theorem 1 is technical. 
For ease of illustration, only the following simplified version will be considered. 

**Theorem 2.** Suppose that $$\mathcal{X}$$, $$\mathcal{Y}$$, and  $$\Gamma$$ are compact sets in Euclidean spaces, and $$\lambda$$ is a continuous function. Then there exists a universally consistent prediction strategy. 

This post is based on [an article by V. Vovk](https://arxiv.org/abs/cs/0606093).

### Probability Forecasting

Intuitively, if we have access to the probability distribution of $y_t$ conditional on all past $x_t$'s and $g_t$'s, making predictions can be easier, as then the prediction protocol is just probabilistically random but not *uncertain*. 
While the protocol above is completely deterministic, it is still possible to do *probability forecasting*, to produce some estimate of the *conditional probability distribution*. 
We will see that such a good probability forecasting strategy permits one to construct a universally consistent prediction strategy. 

The protocol for probability forecasting is as follows. 
For $t = 1, 2, \ldots$, 

1. Reality announces $$x_t \in \mathcal{X}$$. 
2. Forecaster announces $$P_t \in \Delta ( \mathcal{Y} )$$. 
3. Reality announces $y_t \in \mathcal{Y}$.

**Exercise.** Find some real-life examples of the protocol.
A famous one is weather forecasting; see, e.g., [this paper by A. P. Dawid](http://www.jstor.org/stable/2287720 ). 

**Definition.** We say that a forecasting strategy is calibrated, if 

$$
\lim_{T \to \infty} \frac{1}{T} \sum_{t = 1}^T \left( f ( x_t, P_t, y_t ) - \int_{\mathcal{Y}} f ( x_t, P_t, y ) P_t ( \mathrm{d} y ) \right) = 0 , 
$$

for all continuous functions $f: \mathcal{X} \times \Delta ( \mathcal{Y} ) \times \mathcal{Y} \to \mathbb{R}$. 

**Remark.** This is not any of the standard definitions of calibration. 
However, one can observe that it is closely related to the notion of *weak calibration* [proposed by S. M. Kakade and D. P. Foster](https://doi.org/10.1016/j.jcss.2007.04.017), and the notion of *calibration-cum-resolution* [proposed by V. Vovk](https://doi.org/10.1016/j.tcs.2007.07.026). 

**Theorem 3. ([Vovk](https://arxiv.org/abs/cs/0606093))** Suppose that $\mathcal{X}$ and $\mathcal{Y}$ are compact sets in Euclidean spaces. 
Then there exists a calibrated forecasting strategy.

### Proof of Theorem 3

Theorem 3 relies on the fact that for any compact metric space $\Omega$, there always exists some reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ on it, such that the corresponding kernel function $K: \Omega \times \Omega \to \mathbb{R}$ is continuous, the RKHS is dense in $C( \Omega )$, and the embedding constant, defined as

$$
c_{\mathcal{H}} := \sup_{\omega \in \Omega} \sqrt{ K ( \omega, \omega ) } , 
$$

is finite. 

Consider the following probability forecasting strategy. 

*Proof of Theorem 3.*
