
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Que-sais je?</title>
  <meta name="author" content="yhli">

  
  <meta name="description" content="NIPS 2016 was quite successful, in the sense that most of the papers interesting to me were chosen as oral presentations:)
The list below is of &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yenhuanli.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Que-sais je?" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fjalla+One" rel="stylesheet" type="text/css">
<!--- MathJax Configuration -->
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-77006714-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Que-sais je?</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscribe" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS" target="_blank"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="25" height="25" viewbox="0 0 100 100"><path class="social" d="M 13.310204,73.332654 C 5.967347,73.332654 0,79.322448 0,86.621428 c 0,7.338776 5.967347,13.262246 13.310204,13.262246 7.370408,0 13.328572,-5.92245 13.328572,-13.262246 0,-7.29898 -5.958164,-13.288774 -13.328572,-13.288774 z M 0.01530612,33.978572 V 53.143878 C 12.493878,53.143878 24.229592,58.02347 33.068368,66.865306 41.894898,75.685714 46.767346,87.47449 46.767346,100 h 19.25 C 66.017346,63.592858 36.4,33.979592 0.01530612,33.978572 l 0,0 z M 0.03877552,0 V 19.17449 C 44.54796,19.17551 80.77551,55.437756 80.77551,100 H 100 C 100,44.87653 55.15102,0 0.03877552,0 z"></path></svg></a></li>
  
</ul>
  
  
  
  
  
  
  
  
  
  
    
      <form action="https://www.google.com/search" method="get">
        <fieldset role="search">
          <input type="hidden" name="sitesearch" value="yenhuanli.github.io" />
    
          <input class="search" type="text" name="q" results="0" placeholder="Search"/>
        </fieldset>
      </form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/12/12/interesting-talks-in-nips-2016/">Some Interesting Talks in NIPS 2016</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-12-12T22:57:59+01:00'><span class='date'><span class='date-month'>Dec</span> <span class='date-day'>12</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>10:57 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><p>NIPS 2016 was quite successful, in the sense that most of the papers interesting to me were chosen as oral presentations:)
The list below is of course non-exhaustive and biased.
The order is alphabetical, according to the last names of the presenters/first authors.</p>

<ul>
  <li>“Kernel-based Methods for Bandit Convex Optimization” by S. Bubeck
    <ul>
      <li>Bubeck wrote a blog article on this paper: <a href="https://blogs.princeton.edu/imabandit/2016/08/06/kernel-based-methods-for-bandit-convex-optimization-part-1/">part 1</a>, <a href="https://blogs.princeton.edu/imabandit/2016/08/09/kernel-based-methods-for-convex-bandits-part-2/">part 2</a>, and <a href="https://blogs.princeton.edu/imabandit/2016/08/10/kernel-based-methods-for-convex-bandits-part-3/">part 3</a>.</li>
      <li>It seems Bubeck had given basically the same talk in the Simon’s Institute <a href="https://youtu.be/fV4qd43OsY8">(youtube video)</a>.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6490-supervised-learning-through-the-lens-of-compression">“Supervised learning through the lens of compression”</a> by O. David, S. Moran, and A. Yehudayoff
    <ul>
      <li>Roughly speaking, a function class is learnable if it allows (approximate) compression. Notice that the <a href="https://users.soe.ucsc.edu/~manfred/pubs/T1.pdf">compression</a> is not defined in the Shannon-theoretic way.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6467-generalization-of-erm-in-stochastic-convex-optimization-the-dimension-strikes-back">“Generalization of ERM in stochastic convex optimization: The dimension strikes back”</a> by V. Feldman
    <ul>
      <li>This paper shows that minimizing the empirical average is not an optimal strategy for stochastic approximation in general.</li>
    </ul>
  </li>
  <li>“Safe testing: An adaptive alternative to p-value-based testing” by P. Grunwald
    <ul>
      <li>Grunwald proposed the notion of safe test, which is robust against possible abuse of statistical methods, such as collecting data until the p-value is large enough. He also provided an algorithm for safe testing, based on the so-called reverse I-projection.</li>
      <li>Unfortunately, it seems that the paper has not been available on the internet.</li>
    </ul>
  </li>
  <li><a href="https://nips.cc/Conferences/2016/Schedule?showEvent=6206">“Theory and algorithms for forecasting non-stationary time series”</a> by V. Kuznetsov and M. Mohri
    <ul>
      <li>This is a tutorial talk mainly based on their <a href="http://papers.nips.cc/paper/5836-learning-theory-and-algorithms-for-forecasting-non-stationary-time-series">NIPS’15 paper</a> and <a href="http://www.jmlr.org/proceedings/papers/v49/kuznetsov16.html">COLT’16 paper</a>.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6245-without-replacement-sampling-for-stochastic-gradient-methods">“Without-replacement sampling for stochastic gradient methods”</a> by O. Shamir
    <ul>
      <li>This paper provides convergence guarantees for the setting mentioned in its title.</li>
    </ul>
  </li>
  <li><a href="https://nips.cc/Conferences/2016/Schedule?showEvent=6200">“Stochastic optimization: Beyond stochastic gradients and convexity: Part 2”</a> by S. Sra
    <ul>
      <li>The <a href="http://suvrit.de/talks/vr_nips16_sra.pdf">slides</a> can serve as a good bibliography on solving non-convex finite-sum optimization problems.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6268-metagrad-multiple-learning-rates-in-online-learning">“MetaGrad: Multiple learning rates in online learning”</a> by T. van Erven and W. M. Koolen
    <ul>
      <li>This paper proposes a somewhat universally optimal scheme for online learning. The idea is to discretize the interval of possible step sizes, treat each candidate step size as an expert, and then do prediction with experts to choose the step size.</li>
    </ul>
  </li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/11/10/the-pinching-trick-and-the-golden-thompson-inequality/">The Pinching Trick and the Golden-Thompson Inequality</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-11-10T11:10:24+01:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>11:10 am</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="pinching">Pinching</h3>

<p>Let $A$ be a Hermitian matrix, and $A = \sum_j \lambda_j P_j$ be the spectral decomposition of $A$. 
The <em>pinching map</em> defined by $A$ is given by</p>

<script type="math/tex; mode=display">\mathcal{P}_A (X) = \sum_j P_j X P_j ,</script>

<p>for any Hermitian matrix $X$.</p>

<p><strong>Theorem 1.</strong> Let $A$ be a positive semi-definite matrix and $B$ be a Hermitian matrix. 
The following statements hold.</p>

<ol>
  <li>$\mathcal{P}_B (A)$ commutes with $B$.</li>
  <li>$\mathrm{Tr} ( \mathcal{P}_B (A) B ) = \mathrm{Tr} ( A B )$.</li>
  <li><em>(Pinching inequality)</em> $\vert \mathrm{spec} (B) \vert \, \mathcal{P}_B (A) \geq A$, where $\mathrm{spec} (B)$ denotes the set of eigenvalues of $B$.</li>
</ol>

<p>The first two statements are easy to check.
The earliest reference on the pinching inequality I can find is the <a href="https://www.elsevier.com/books/von-neumann-algebras/dixmier/978-0-444-86308-9">classic book by Jacques Dixmier</a>.
A simple proof of the pinching inequality can be found in the <a href="http://www.springer.com/us/book/9783540302650">textbook by Masahito Hayashi</a>.</p>

<p>One main issue in matrix analysis is non-commutativity. 
The first statement in Theorem 1 hints that pinching can be an useful tool to deal with this issue. 
In the next section, the pinching trick is illustrated using the Golden-Thompson inequality as an example.</p>

<h3 id="a-proof-of-the-golden-thompson-inequality">A proof of the Golden-Thompson Inequality</h3>

<p>The <a href="https://en.wikipedia.org/wiki/Golden%E2%80%93Thompson_inequality">Golden-Thompson inequality</a> says that</p>

<script type="math/tex; mode=display">\mathrm{Tr} ( \exp ( A + B ) ) \leq \mathrm{Tr} ( \exp (A) \exp (B) ) ,</script>

<p>for any two Hermitian matrices $A$ and $B$. 
Obviously, if $A$ commutes with $B$, the Golden-Thompson inequality holds with an equality; however, in general one needs to take non-commutativity into consideration.
Below we present a very elegant proof using the pinching trick from a <a href="https://arxiv.org/abs/1604.03023">recent paper by D. Sutter et al</a>.</p>

<p>The key observation is that $\vert \mathrm{spec} ( A^{\otimes n} ) \vert$ does not grow rapidly with $n$ for any Hermitian matrix $A$.</p>

<p><strong>Lemma 1.</strong> One has $\vert \mathrm{spec} ( A^{\otimes n} ) \vert = O ( \mathsf{poly} (n) )$ for any Hermitian matrix $A$.</p>

<p><em>Proof (Golden-Thompson inequality).</em></p>

<p>Let $X$ and $Y$ be two positive definite matrices. 
Then one can write</p>

<script type="math/tex; mode=display">\log \mathrm{Tr} ( \exp ( \log X + \log Y ) ) = \frac{1}{n} \log \mathrm{Tr} ( \exp ( \log X^{\otimes n} + \log Y^{\otimes n} ) ),</script>

<p>for any positive integer $n$. 
By the pinching inequality, one has</p>

<script type="math/tex; mode=display">\frac{1}{n} \log \mathrm{Tr} ( \exp ( \log X^{\otimes n} + \log Y^{\otimes n} ) ) \leq \frac{1}{n} \log \mathrm{Tr} \{ \exp [ \log ( \vert \mathrm{spec} ( Y^{\otimes n} ) \vert \, \mathcal{P}_{Y^{\otimes n}} ( X^{\otimes n} ) ] + \log Y^{\otimes n} ) \}</script>

<p>By the first two statements in Theorem 1 and Lemma 1, one has</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{RHS} & = \frac{1}{n} \log \mathrm{Tr}\, ( \mathcal{P}_{Y^{\otimes n}} ( X^{\otimes n} ) Y^{\otimes n} ) + \frac{\log \mathsf{poly} (n)}{n} \notag \\
& = \frac{1}{n} \log \mathrm{Tr} ( X^{\otimes n} Y^{\otimes n} ) + \frac{\log \mathsf{poly} (n)}{n} \notag \\
& = \log \mathrm{Tr}\, ( X Y ) + \frac{\log \mathsf{poly} (n)}{n} . 
\end{align} %]]></script>

<p>Then one obtains the Golden-Thompson Inequality by letting $n \to \infty$.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/11/09/pbineq/">Peierls-Bogoliubov Inequality</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-11-09T23:39:18+01:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>9</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>11:39 pm</span></time>
        
        
      </p>
    
  </header>


  <div class="entry-content"><p>The term <em>Peierls-Bogoliubov inequality</em> in fact refers to convexity of the function:</p>

<script type="math/tex; mode=display">\begin{equation}
\varphi (t) = \log \mathrm{Tr}\, \exp ( A + t B ) , \quad t \in \mathbb{R} .
\end{equation}</script>

<p>for any two given Hermitian matrices $A$ and $B$.</p>

<p>Let $B = \sum_{j \in \mathcal{J}} \lambda_i P_i$ be the spectral decomposition of $B$. 
Consider the random variable $\xi_t$ satisfying</p>

<script type="math/tex; mode=display">\begin{equation}
\mathsf{P} ( \xi_t = \lambda_j ) = \frac{ \mathrm{Tr} ( P_i \exp ( A + t B ) ) }{\mathrm{Tr}\, \exp ( A + t B )}, \quad \text{for all } j \in \mathcal{J} .
\end{equation}</script>

<p>Then it can be easily verified that</p>

<script type="math/tex; mode=display">\begin{equation}
\varphi' (t) = \mathsf{E}\, \xi_t , \quad \varphi'' (t) = \mathsf{E}\, \xi_t ^ 2 - ( \mathsf{E}\, \xi_t )^2 = \mathsf{var} ( \xi_t ) .
\end{equation}</script>

<p>Therefore, $\varphi’’ (t) \geq 0$ for all $t$, which implies the following theorem.</p>

<p><strong>Theorem.</strong> The function $\varphi$ is convex.</p>

<p>The following Peierls-Bogoliubov inequality in the <a href="http://math.arizona.edu/events/AZschool/material/AZ09-carlen.pdf">lecture notes by Eric Carlen</a> immediately follows.</p>

<p><strong>Corollary.</strong> The mapping $A \mapsto \log \mathrm{Tr}\, \exp (A)$ is convex.</p>

<p>The arguably <a href="https://en.wikipedia.org/wiki/Trace_inequalities#Peierls.E2.80.93Bogoliubov_inequality">most well-known version</a> of the Peierls-Bogoliubov inequality also follows from the convexity of $\varphi$.</p>

<p><strong>Corollary.</strong> It holds that $\varphi(1) \geq \varphi(0) + \varphi’(0) \cdot ( 1 - 0 )$, i.e.,</p>

<script type="math/tex; mode=display">\begin{equation}
\log \mathrm{Tr}\, \exp ( A + B ) \geq \log \mathrm{Tr}\, \exp ( A ) + \frac{ \mathrm{Tr} ( B \exp ( A )  }{ \mathrm{Tr}\, \exp ( A ) } . 
\end{equation}</script>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    
  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 - yhli -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
