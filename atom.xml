<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Que-sais je?]]></title>
  <link href="http://yenhuanli.github.io/atom.xml" rel="self"/>
  <link href="http://yenhuanli.github.io/"/>
  <updated>2017-03-20T16:49:16+01:00</updated>
  <id>http://yenhuanli.github.io/</id>
  <author>
    <name><![CDATA[yhli]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hedge Algorithm (1/2)]]></title>
    <link href="http://yenhuanli.github.io/blog/2017/03/19/hedge/"/>
    <updated>2017-03-19T11:56:20+01:00</updated>
    <id>http://yenhuanli.github.io/blog/2017/03/19/hedge</id>
    <content type="html"><![CDATA[<p>Alice wants to win some money by betting on horce racing games. 
She knows nothing about horce racing. 
Fortunately, she knows that some of her friends are very good at betting on horce racing; however, she does not know who exactly is the best.
Suppose Alice will bet on several rounds sequentially.
Can Alice bet as well as the best among her friends <em>in the long run</em>?</p>

<p>Indeed, there are algorithms for doing so. 
A famous one is the so-called <em>hedge algorithm</em>, introduced in the paper <a href="http://dx.doi.org/10.1006/jcss.1997.1504">“A desicion-theoretic generalization of on-line learning and an application to boosting”</a> by Y. Freund and R. Schapire.</p>

<p>The idea is to switch between Alice’s fridends’ strategies. 
Assume for simplicity that Alice will always bet 1 unit of money for each round. 
Alice keeps in her mind a <em>weight</em> for each of her friend. 
At the beginning of each round, she chooses one of her friends with probability proportional to the weights, say Bob, and follows Bob’s bet. 
Once a round finishes, Alice sees how much she would lose had she followed any of her friend. 
She then updates the weights accordingly.
Those who yielded smaller losses have higher weights, and vice versa.</p>

<p>Let us formalize the idea. 
We label the rounds by positive integers $t \in \mathbb{N}$.
Suppse Alice has $N$ friends; let us index them by the set $[N] := \lbrace 1, \ldots, N \rbrace$.
We summarize the weights before the $t$-th round by a weight vector $w_t := ( w_t (i) )_{i \in [N]} \in \mathbb{R}^N$ in the probability simplex, where $w_t (i)$ denotes the weight assigned to Friend $i$.
We also summarize the losses yielded by each friend’s bet for the $t$-th round by a vector</p>

<script type="math/tex; mode=display">x_t := ( x_t (i) )_{i \in [N]} \in [ 0, + \infty [^N .</script>

<p>Then for the $t$-th round, the expected loss of Alice is</p>

<script type="math/tex; mode=display">\sum_{i \in [N] } x_t(i) w_t(i) := \langle x_t, w_t \rangle .</script>

<p>Suppose Alice will bet on $T$ rounds.
The goal of Alice is to have a small <em>regret</em></p>

<script type="math/tex; mode=display">R_T := \sum_{t \in [T]} \langle x_t, w_t \rangle - \min_{i \in [N]} \sum_{t \in [T]} x_t(i) ,</script>

<p>where $[T] := \lbrace 1, \ldots, T \rbrace$.</p>

<p>The regret measures the difference between Alice’s expected loss and the loss yielded by the best friend <em>in hindsight</em>.
If $R_T = o(T)$, in hindsight, the difference between the average expected costs of Alice and the best friend converges to zero. 
Notice that the <em>best friend</em> in general varies with $T$.</p>

<p>The hedge algorithm is a specific rule of setting the weights. 
Initially, Alice chooses any weight $w_1$ such that $w_1 (i) \neq 0$ for all $i$. 
After observing the losses $x_t$, Alice updates the weights as</p>

<script type="math/tex; mode=display">w_{t+1} (i) = Z_t^{-1} w_t(i) \, \mathrm{e}^{- \eta x_t (i)} , \quad i \in [N] ,</script>

<p>where $Z_t := \sum_{i \in [N]} w_t (i) \, \mathrm{e}^{- \eta x_t (i)}$ is the normalizing constant, and $\eta$ is a properly chosen <em>learning rate</em>.</p>

<p>The hedge algorithm looks similar to the aggregating algorithm (AA). 
Indeed, the former can be viewed as an instance of the latter. 
Recall the setting of prediction with expert advice (PEA). 
Let Alice be Learner and her friends be the experts.
Let Nature choose the loss vectors.
Set the outcome space $\Omega$ as $[ 0, + \infty [^N$, prediction space $\Gamma$ as the probability simplex in $\mathbb{R}^N$, and loss function as</p>

<script type="math/tex; mode=display">\lambda ( \omega, \gamma ) := \langle \omega, \gamma \rangle , \quad \omega \in \Omega, \gamma \in \Gamma.</script>

<p>When each expert $i \in [N]$ chooses a fixed prediction $\gamma_t (i) = ( \delta_{i,j} )_{j \in [N]} \in \mathbb{R}^N$, we recover the original betting-horce-racing-games problem.
It is easy to check that applying the AA on this specific PEA game recovers the hedge algorithm.</p>

<p>Unfortunately, this specific PEA game does not satisfy the mixability condition, so we cannot apply the regret bound in <a href="http://yenhuanli.github.io/blog/2017/03/02/aggregating-algorithm/">this post on the AA</a>.
We will see that by slightly generalizing the mixability condition, it is not difficult to show that the hedge algorithm yields $R_T = O (\sqrt{T})$, assuming bounded losses.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weak Aggregation]]></title>
    <link href="http://yenhuanli.github.io/blog/2017/03/17/weak-aggregation/"/>
    <updated>2017-03-17T16:53:40+01:00</updated>
    <id>http://yenhuanli.github.io/blog/2017/03/17/weak-aggregation</id>
    <content type="html"><![CDATA[<p>The <em>weak aggregating algorithm (WAA)</em> was introduced in the paper <a href="http://dx.doi.org/10.1016/j.jcss.2007.08.003">“The weak aggregating algorithm and weak mixability”</a> by Kalnishkan and V’yugin. 
Unlike the aggregating algorithm (AA) by Vovk, the WAA does not require the mixability condition but a weaker convexity condition which will appear shortly.</p>

<h3 id="weak-aggregating-algorithm">Weak Aggregating Algorithm</h3>

<p>Recall the <em>prediction with expert advice (PEA)</em> game, which involves <em>Learner</em>, <em>Nature</em>, and a pool of experts indexed by a set $\Theta$. 
Let $\Omega$ be the <em>outcome space</em>, $\Gamma$ be the <em>prediction space</em>, and $\lambda: \Omega \times \Gamma \to [ 0, + \infty )$ be a <em>loss function</em>.
For each $t \in \mathbb{N}$,</p>

<ol>
  <li>Each expert $\theta \in \Theta$ chooses a prediction $\gamma_t ( \theta ) \in \Gamma$.</li>
  <li>Learner chooses a prediction $\gamma_t \in \Gamma$, which can be dependent on all previous predictions by himself and the experts, and previous outcomes chosen by Nature.</li>
  <li>Nature chooses $\omega_t \in \Omega$.</li>
  <li>Each expert $\theta \in \Theta$ suffers for the loss $\lambda ( \omega_t, \gamma_t ( \theta ) )$. Learner suffers for the loss $\lambda ( \omega_t, \gamma_t )$.</li>
</ol>

<p>The WAA works as follows. 
Let $( \eta_t )_{t \in \mathbb{N}}$ be a given sequence of <em>learning rates</em>.
Let $P_0$ be a probability measure on $\Theta$.
For each $t \in \mathbb{N}$, Learner chooses as his prediction any $\gamma_t$ such that</p>

<script type="math/tex; mode=display">\lambda( \omega_t, \gamma_t ) \leq \int_\Theta \lambda ( \omega_t, \gamma_t (\theta) ) \, P_{t-1} ( \mathrm{d} \theta ) .</script>

<p>After observing the losses, Learner constructs a probability measure $P_t$ such that for any $\mathcal{A} \subseteq \Theta$</p>

<script type="math/tex; mode=display">P_t ( \mathcal{A} ) = \int_{\mathcal{A}} \exp \left( - \eta_{t + 1} L_t ( \theta ) \right) \, P_0 ( \mathrm{d} \theta ) ,</script>

<p>where $L_t ( \theta )$ denotes the <em>accumulative loss</em> of expert $\theta$, i.e.,</p>

<script type="math/tex; mode=display">L_t ( \theta ) := \lambda ( \omega_1, \gamma_1 ( \theta ) ) + \lambda ( \omega_2, \gamma_2 ( \theta ) ) + \cdots + \lambda ( \omega_t, \gamma_t ( \theta ) ) .</script>

<p>Of course, the rule of choosing $\gamma_t$ does not apply to any PEA game.
Hence we need to require the <em>convexity</em> of the game.</p>

<p><strong>Definition.</strong>
The game is said to be convex, if for any probability measure $P$ on $\Gamma$, there always exists some $\gamma^\star \in \Gamma$ such that</p>

<script type="math/tex; mode=display">\lambda ( \omega, \gamma^\star ) \leq \int_\Gamma \lambda ( \omega, \gamma ) \, P ( \mathrm{d} \gamma ) , \quad \text{for any } \omega \in \Omega .</script>

<p>By Jensen’s inequality, if $\Gamma$ is convex and the mapping $\gamma \mapsto \lambda ( \omega, \gamma )$ is also convex for all $\omega \in \Omega$, then one can simply choose</p>

<script type="math/tex; mode=display">\gamma^\star = \int_\Gamma \gamma \, P ( \mathrm{d} \gamma ) .</script>

<p>If a game is $\eta$-mixable for some $\eta &gt; 0$, then it is convex.</p>

<h3 id="regret-analysis">Regret Analysis</h3>

<p>For any $T \in \mathbb{N}$, define the accumulative loss of Learner as</p>

<script type="math/tex; mode=display">L_T := \lambda ( \omega_1, \gamma_1 ) + \lambda ( \omega_2, \gamma_2 ) + \cdots + \lambda ( \omega_T, \gamma_T ) .</script>

<p><strong>Lemma.</strong> Assume that the sequence $( \eta_t )_{t \in \mathbb{N}}$ is non-increasing. 
For every $T \in \mathbb{N}$, one has</p>

<script type="math/tex; mode=display">\exp \left( -\eta_T L_T \right) \geq \exp \left( - \eta_T \sum_{t \leq T} \delta_t \right) \int_{\Theta} \exp \left( - \eta_T L_T ( \theta ) \right) \, P_0 ( \mathrm{d} \theta ) ,</script>

<p>where</p>

<script type="math/tex; mode=display">\delta_t := \int_\Theta \lambda ( \omega_t, \gamma_t (\theta) ) \, P_{t - 1} ( \mathrm{d} \theta ) + \eta_t^{-1} \log \int_\Theta \exp \left( - \eta_t \lambda ( \omega_t, \gamma_t (\theta) ) \right) \, P_{t - 1} ( \mathrm{d} \theta ) .</script>

<p><strong>Proof.</strong>
We prove by induction.
Assume the lemma holds for $t = T \in \mathbb{N}$.
For $t = T + 1$, one writes</p>

<script type="math/tex; mode=display">\exp \left( - \eta_{T + 1} L_{T + 1} \right) = \exp \left( - \eta_{T + 1} L_T \right) \exp \left( - \eta_{T + 1} \lambda ( \omega_{T + 1}, \gamma_{T + 1} ) \right) .</script>

<p>Using the induction hypothesis, one can write</p>

<script type="math/tex; mode=display">\exp \left( - \eta_{T + 1} L_T \right) = \left[ \exp \left( - \eta_T L_T \right) \right]^{\frac{\eta_{T + 1}}{\eta_T}} \geq \left[ \exp \left( - \eta_T \sum_{t \leq T} \delta_T \right) \int_\Theta \exp \left( - \eta_T L_T ( \theta ) \right) \, P_0 ( \mathrm{d} \theta ) \right]^{\frac{\eta_{T + 1}}{\eta_T}} .</script>

<p>By Jensen’s inequality, the RHS is bounded below by</p>

<script type="math/tex; mode=display">\exp \left( - \eta_{T + 1} \sum_{t \leq T} \delta_t \right) \int_\Theta \exp \left( - \eta_{T + 1} L_T (\theta) \right) \, P_0 (\mathrm{d} \theta) .</script>

<p>By the rule of choosing $\gamma_{T + 1}$, one has</p>

<script type="math/tex; mode=display">\exp \left( - \eta_{T + 1} \lambda (\omega_{T + 1}, \gamma_{T + 1}) \right) \geq \exp \left( - \eta_{T + 1} \int_\Theta \lambda (\omega_{T + 1}, \gamma_{T + 1} ( \theta )) \, P_T ( \mathrm{d} \theta ) \right) .</script>

<p>One can write the RHS as</p>

<script type="math/tex; mode=display">\exp \left( - \eta_{T + 1} \delta_{T + 1} \right) \int_\Theta \exp \left( - \eta_{T + 1} \lambda ( \omega_{T + 1}, \gamma_{T + 1} ( \theta ) ) \right) \, P_T ( \mathrm{d} \theta ) .</script>

<p>The lemma follows, as</p>

<script type="math/tex; mode=display">\int_\Theta \exp \left( - \eta_{T + 1} \lambda ( \omega_{T + 1}, \gamma_{T + 1} ( \theta ) ) \right) \, P_T ( \mathrm{d} \theta ) = \frac{ \int_\Theta \exp \left( - \eta_{T + 1} \lambda ( \omega_{T + 1}, \gamma_{T + 1} (\theta) ) \right) \exp \left( - \eta_{T + 1} L_T ( \theta ) \right) \, P_0 ( \mathrm{d} \theta ) }{ \int_\Theta \exp \left( - \eta_{T + 1} L_T (\theta) \right) \, P_0 (\mathrm{d} \theta) } .</script>

<p><em>Q.E.D.</em></p>

<p>Let us use the lemma to derive a regret bound under the following two assumptions.</p>

<ol>
  <li>The cardinality of $\Theta$ is finite.</li>
  <li>The loss $\lambda ( \omega, \gamma )$ is bounded above by some $M &gt; 0$, for all $\omega \in \Omega$ and $\gamma \in \Gamma$.</li>
</ol>

<p>The lemma implies, for any $T \in \mathbb{N}$,</p>

<script type="math/tex; mode=display">L_T \leq L_T ( \theta ) + \eta_T^{-1} \log ( 1 / P_0 ( \theta ) ) + \sum_{t \leq T} \delta_t , \quad \text{for all } \theta \in \Theta.</script>

<p>It remains to bound the term $\sum_{t \leq T} \delta_t$.
For every $t \in \mathbb{N}$, define the random variable $X_t := - \lambda ( \omega_t, \gamma_t ( \theta ) )$.
Using Hoeffding’s lemma, one obtians</p>

<script type="math/tex; mode=display">\delta_t = - \mathbb{E}\, X_t + \eta_t^{-1} \log \mathbb{E}\, \exp ( \eta_t X_t ) \leq \frac{\eta_t M^2}{8} , \quad \text{for all } t \in \mathbb{N}.</script>

<p>Therefore,</p>

<script type="math/tex; mode=display">L_T \leq L_T ( \theta ) + \eta_T^{-1} \log ( 1 / P_0 ( \theta ) ) + \frac{M^2}{8} \sum_{t \leq T} \eta_t .</script>

<p>Choosing $\eta_t = c / \sqrt{t}$ for some $c &gt; 0$ yields</p>

<script type="math/tex; mode=display">\sum_{t \leq T} \eta_t = c \sum_{t \leq T} \frac{1}{\sqrt{t}} \leq c \int_{[0,T]} \frac{1}{\sqrt{x}} \, \mathrm{d} x = 2 c \sqrt{T}.</script>

<p>Minimizing the bound with respect to $c$, one obtains the following regret bound.</p>

<p><strong>Theorem.</strong> Suppose that $\vert \Theta \vert = N &lt; + \infty$, and the loss function is bounded above by $M &gt; 0$. Then the WAA with $P_0 ( \theta ) = 1 / N$ for all $\theta$ and</p>

<script type="math/tex; mode=display">\eta_t = \frac{2}{M} \sqrt{ \frac{\log N}{t} }</script>

<p>achieves, for all $T \in \mathbb{N}$,</p>

<script type="math/tex; mode=display">L_T \leq L_T (\theta) + M \sqrt{ T \log N } , \quad \text{for all } \theta \in \Theta.</script>

<p><strong>Remark.</strong> We have used Hoeffding’s lemma to simplify a little bit the original proof.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aggregating Algorithm]]></title>
    <link href="http://yenhuanli.github.io/blog/2017/03/02/aggregating-algorithm/"/>
    <updated>2017-03-02T18:57:06+01:00</updated>
    <id>http://yenhuanli.github.io/blog/2017/03/02/aggregating-algorithm</id>
    <content type="html"><![CDATA[<p>This post is essentially a summary of some results in the classical paper <a href="http://dx.doi.org/10.1006/jcss.1997.1556">“A game of prediction with expert advice”</a> by V. Vovk.</p>

<p><strong>Definition.</strong> A <em>game</em> is a triple $( \Omega, \Gamma, \lambda )$, where $\Omega$ is the <em>outcome space</em>, $\Gamma$ is the <em>prediction space</em>, and $\lambda: \Omega \times \Gamma \to [0, \infty]$ is the <em>loss function</em>.</p>

<p>Consider the standard <em>learning with expert advice</em> setting, in which a <em>learner</em> tries to predict the outcomes of the <em>nature</em> sequentially, with the help of $n$ <em>experts</em>. 
Precisely speaking, at each trial $t \in \mathbb{N}$:</p>

<ol>
  <li>Each expert $i$ makes a prediction $\gamma_t (i)$, $1 \leq i \leq n$.</li>
  <li>The learner makes a prediction $\gamma_t \in \Gamma$.</li>
  <li>The nature chooses an outcome $\omega_t \in \Omega$.</li>
  <li>The learner suffers for the loss $\lambda ( \omega_t, \gamma_t )$.</li>
</ol>

<p>Notice that before making his prediction at the trial $t$, the learner has access to the past history of the nature (up to trial $t - 1$) and all of the experts (up to trial $t$).</p>

<p>For every $t \in \mathbb{N}$, let us define the learner’s accumulative loss as</p>

<script type="math/tex; mode=display">L_t := \lambda( \omega_1, \gamma_1 ) + \cdots + \lambda ( \omega_t, \gamma_t ),</script>

<p>and each expert’s accumulative loss as</p>

<script type="math/tex; mode=display">L_t ( i ) := \lambda( \omega_1, \gamma_1 ( i ) ) + \cdots + \lambda( \omega_t, \gamma_t ( i ) ), \quad i = 1, \ldots, n .</script>

<p>The learner’s goal is to make predictions almost as well as the <em>best expert</em>, in the sense that $L_t \leq L_t(i) + \varepsilon$ for all $i \leq n$, $t \in \mathbb{N}$, and some <em>small enough</em> $\varepsilon$.</p>

<p><strong>Definition.</strong> We say the game is <em>$\eta$-mixable</em>, if for any probability distribution $P$ on $\Gamma$, there exists a $\gamma^\star \in \Gamma$, such that</p>

<script type="math/tex; mode=display">\exp ( - \eta \lambda ( \omega, \gamma^\star ) ) \geq \int \exp ( - \eta \lambda( \omega, \gamma ) ) \, P( \mathrm{d} \gamma ), \quad \text{for all } \omega \in \Omega . \notag</script>

<p><strong>Remark.</strong> By Jensen’s inequality, if the mapping $\gamma \mapsto \exp ( - \eta \lambda ( \omega, \gamma ) )$ is concave for all $\omega \in \Omega$, then the game is $\eta$-mixable.
Any such loss function $\lambda$ is called <em>exp-concave</em>. 
Obviously, one can simply choose $\gamma^\star = \int \gamma \, P ( \mathrm{d} \gamma )$ if the loss is exp-concave.</p>

<p><strong>Proposition.</strong> <em>If a game is $\eta$-mixable, there exists a prediction algorithm for the learner, such that</em></p>

<script type="math/tex; mode=display">L_t \leq L_t ( i ) + \eta^{-1} \ln n , \quad \text{for all } i \leq n .</script>

<p>One such algorithm is the <em>aggregating algorithm (AA)</em>, first introduced in the paper <a href="http://vovk.net/aa/index.html">“Aggregating strategies”</a> by V. Vovk.
Let $( \pi_0 ( i ) )_{i \leq n}$ be a probability vector whose entries are all non-zero. 
At each trial $t \in \mathbb{N}$, the AA outputs a prediction $\gamma_t \in \Gamma$ satisfying</p>

<script type="math/tex; mode=display">\exp ( - \eta \lambda ( \omega_t, \gamma_t ) ) \geq \sum_{i \leq n} \pi_{t - 1} ( i ) \exp ( - \eta \lambda( \omega_t, \gamma_t ( i ) ) ) ,</script>

<p>and after observing $\omega_t$, the AA updates the probability vector as</p>

<script type="math/tex; mode=display">\pi_t ( i ) := z_t^{-1} \pi_{t - 1} ( i ) \exp ( - \eta \, \lambda ( \omega_t, \gamma_t ( i ) ) ) , \quad \text{for all } i \leq n ,</script>

<p>where $z_t$ is a normalizing constant such that $( \pi_t (i) )_{i \leq n}$ is also a probability vector.
Because of the $\eta$-mixability assumption, the AA is well-defined.</p>

<p><strong>Lemma.</strong> For each $t \in \mathbb{N}$, one has</p>

<script type="math/tex; mode=display">\exp ( - \eta L_t ) \geq \sum_{i \leq n} \pi_0 ( i ) \, \exp ( - \eta L_t ( i ) ) .</script>

<p><strong>Proof.</strong> We prove by induction. 
Obviously, the inequality holds for $t = 0$ (for which we set $L_t = L_t (i) = 0$ for all $i \leq n$).
Assume the inequality to be proved holds for $t = T$.
We write</p>

<script type="math/tex; mode=display">\exp ( - \eta L_{T + 1} ) = \exp ( - \eta L_T ) \exp ( - \eta \lambda ( \omega_{T + 1}, \gamma_{T + 1}(i) ) )</script>

<p>By the definition of the AA, the RHS is bounded below by</p>

<script type="math/tex; mode=display">\exp ( - \eta L_T ) \frac{ \sum_{i \leq n} \exp ( - \eta \lambda ( \omega_{T+1}, \lambda_{T+1} ( i ) ) ) \, \pi_0 ( i ) \, \exp ( - \eta L_T ( i ) ) }{ \sum_{i \leq n} \pi_0 ( i ) \, \exp ( - \eta L_T ( i ) ) } .</script>

<p>By assumption, the RHS can be further bounded below by</p>

<script type="math/tex; mode=display">\sum_{i \leq n} \exp ( - \eta \lambda ( \omega_{T+1}, \lambda_{T+1} ( i ) ) ) \, \pi_0 ( i ) \, \exp ( - \eta L_{T} ( i ) ) = \sum_{i \leq n} \pi_0 ( i ) \exp ( \eta L_{T+1} ( i ) ) .</script>

<p>This proves the lemma. <em>Q.E.D.</em></p>

<p>By the lemma, we have</p>

<script type="math/tex; mode=display">\exp ( - \eta L_t ) \geq \pi_0 ( i ) \, \exp ( - \eta L_t ( i ) ) , \quad \text{for all } i \leq n .</script>

<p>Hence we obtain</p>

<script type="math/tex; mode=display">L_t \leq L_t ( i ) + \frac{1}{ \eta } \log \left( \frac{1}{ \pi_0 ( i ) } \right) , \quad \text{for all} i \leq n.</script>

<p>Choosing $\pi_0 ( i ) = 1 / n$ for all $i$ (the uniform distribution) proves the proposition.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Some Interesting Talks at NIPS 2016]]></title>
    <link href="http://yenhuanli.github.io/blog/2016/12/12/interesting-talks-in-nips-2016/"/>
    <updated>2016-12-12T22:57:59+01:00</updated>
    <id>http://yenhuanli.github.io/blog/2016/12/12/interesting-talks-in-nips-2016</id>
    <content type="html"><![CDATA[<p>NIPS 2016 was quite successful, in the sense that most of the papers interesting to me were chosen as oral presentations:)
The list below is of course non-exhaustive and biased.
The order is alphabetical, according to the last names of the presenters/first authors.</p>

<ul>
  <li>“Kernel-based Methods for Bandit Convex Optimization” by S. Bubeck
    <ul>
      <li>Bubeck wrote a blog article on this paper: <a href="https://blogs.princeton.edu/imabandit/2016/08/06/kernel-based-methods-for-bandit-convex-optimization-part-1/">part 1</a>, <a href="https://blogs.princeton.edu/imabandit/2016/08/09/kernel-based-methods-for-convex-bandits-part-2/">part 2</a>, and <a href="https://blogs.princeton.edu/imabandit/2016/08/10/kernel-based-methods-for-convex-bandits-part-3/">part 3</a>.</li>
      <li>Bubeck had given basically the same talk at the Simons Institute <a href="https://youtu.be/fV4qd43OsY8">(youtube video)</a>.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6490-supervised-learning-through-the-lens-of-compression">“Supervised learning through the lens of compression”</a> by O. David, S. Moran, and A. Yehudayoff
    <ul>
      <li>Roughly speaking, a function class is learnable if it allows (approximate) compression. Notice that the <a href="https://users.soe.ucsc.edu/~manfred/pubs/T1.pdf">compression</a> is not defined in the Shannon-theoretic way.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6467-generalization-of-erm-in-stochastic-convex-optimization-the-dimension-strikes-back">“Generalization of ERM in stochastic convex optimization: The dimension strikes back”</a> by V. Feldman
    <ul>
      <li>This paper shows that minimizing the empirical average is not an optimal strategy for stochastic approximation in general.</li>
    </ul>
  </li>
  <li>“Safe testing: An adaptive alternative to p-value-based testing” by P. Grunwald
    <ul>
      <li>Grunwald proposed the notion of safe test, which is robust against possible abuse of statistical methods, such as collecting data until the p-value is large enough. He also provided an algorithm for safe testing, based on the so-called reverse I-projection.</li>
      <li>Unfortunately, it seems that the paper has not been available on the internet.</li>
    </ul>
  </li>
  <li><a href="https://nips.cc/Conferences/2016/Schedule?showEvent=6206">“Theory and algorithms for forecasting non-stationary time series”</a> by V. Kuznetsov and M. Mohri
    <ul>
      <li>This is a tutorial talk mainly based on their <a href="http://papers.nips.cc/paper/5836-learning-theory-and-algorithms-for-forecasting-non-stationary-time-series">NIPS’15 paper</a> and <a href="http://www.jmlr.org/proceedings/papers/v49/kuznetsov16.html">COLT’16 paper</a>.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6245-without-replacement-sampling-for-stochastic-gradient-methods">“Without-replacement sampling for stochastic gradient methods”</a> by O. Shamir
    <ul>
      <li>This paper provides convergence guarantees for the setting mentioned in its title.</li>
    </ul>
  </li>
  <li><a href="https://nips.cc/Conferences/2016/Schedule?showEvent=6200">“Stochastic optimization: Beyond stochastic gradients and convexity: Part 2”</a> by S. Sra
    <ul>
      <li>The <a href="http://suvrit.de/talks/vr_nips16_sra.pdf">slides</a> can serve as a good bibliography on solving non-convex finite-sum optimization problems.</li>
    </ul>
  </li>
  <li><a href="http://papers.nips.cc/paper/6268-metagrad-multiple-learning-rates-in-online-learning">“MetaGrad: Multiple learning rates in online learning”</a> by T. van Erven and W. M. Koolen
    <ul>
      <li>This paper proposes a somewhat universally optimal scheme for online learning. The idea is to discretize the interval of possible step sizes, treat each candidate step size as an expert, and then do prediction with experts to choose the step size.</li>
    </ul>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Pinching Trick and the Golden-Thompson Inequality]]></title>
    <link href="http://yenhuanli.github.io/blog/2016/11/10/the-pinching-trick-and-the-golden-thompson-inequality/"/>
    <updated>2016-11-10T11:10:24+01:00</updated>
    <id>http://yenhuanli.github.io/blog/2016/11/10/the-pinching-trick-and-the-golden-thompson-inequality</id>
    <content type="html"><![CDATA[<h3 id="pinching">Pinching</h3>

<p>Let $A$ be a Hermitian matrix, and $A = \sum_j \lambda_j P_j$ be the spectral decomposition of $A$. 
The <em>pinching map</em> defined by $A$ is given by</p>

<script type="math/tex; mode=display">\mathcal{P}_A (X) = \sum_j P_j X P_j ,</script>

<p>for any Hermitian matrix $X$.</p>

<p><strong>Theorem 1.</strong> Let $A$ be a positive semi-definite matrix and $B$ be a Hermitian matrix. 
The following statements hold.</p>

<ol>
  <li>$\mathcal{P}_B (A)$ commutes with $B$.</li>
  <li>$\mathrm{Tr} ( \mathcal{P}_B (A) B ) = \mathrm{Tr} ( A B )$.</li>
  <li><em>(Pinching inequality)</em> $\vert \mathrm{spec} (B) \vert \, \mathcal{P}_B (A) \geq A$, where $\mathrm{spec} (B)$ denotes the set of eigenvalues of $B$.</li>
</ol>

<p>The first two statements are easy to check.
The earliest reference on the pinching inequality I can find is the <a href="https://www.elsevier.com/books/von-neumann-algebras/dixmier/978-0-444-86308-9">classic book by Jacques Dixmier</a>.
A simple proof of the pinching inequality can be found in the <a href="http://www.springer.com/us/book/9783540302650">textbook by Masahito Hayashi</a>.</p>

<p>One main issue in matrix analysis is non-commutativity. 
The first statement in Theorem 1 hints that pinching can be an useful tool to deal with this issue. 
In the next section, the pinching trick is illustrated using the Golden-Thompson inequality as an example.</p>

<h3 id="a-proof-of-the-golden-thompson-inequality">A proof of the Golden-Thompson Inequality</h3>

<p>The <a href="https://en.wikipedia.org/wiki/Golden%E2%80%93Thompson_inequality">Golden-Thompson inequality</a> says that</p>

<script type="math/tex; mode=display">\mathrm{Tr} ( \exp ( A + B ) ) \leq \mathrm{Tr} ( \exp (A) \exp (B) ) ,</script>

<p>for any two Hermitian matrices $A$ and $B$. 
Obviously, if $A$ commutes with $B$, the Golden-Thompson inequality holds with an equality; however, in general one needs to take non-commutativity into consideration.
Below we present a very elegant proof using the pinching trick from a <a href="https://arxiv.org/abs/1604.03023">recent paper by D. Sutter et al</a>.</p>

<p>The key observation is that $\vert \mathrm{spec} ( A^{\otimes n} ) \vert$ does not grow rapidly with $n$ for any Hermitian matrix $A$.</p>

<p><strong>Lemma 1.</strong> One has $\vert \mathrm{spec} ( A^{\otimes n} ) \vert = O ( \mathsf{poly} (n) )$ for any Hermitian matrix $A$.</p>

<p><em>Proof (Golden-Thompson inequality).</em></p>

<p>Let $X$ and $Y$ be two positive definite matrices. 
Then one can write</p>

<script type="math/tex; mode=display">\log \mathrm{Tr} ( \exp ( \log X + \log Y ) ) = \frac{1}{n} \log \mathrm{Tr} ( \exp ( \log X^{\otimes n} + \log Y^{\otimes n} ) ),</script>

<p>for any positive integer $n$. 
By the pinching inequality, one has</p>

<script type="math/tex; mode=display">\frac{1}{n} \log \mathrm{Tr} ( \exp ( \log X^{\otimes n} + \log Y^{\otimes n} ) ) \leq \frac{1}{n} \log \mathrm{Tr} \{ \exp [ \log ( \vert \mathrm{spec} ( Y^{\otimes n} ) \vert \, \mathcal{P}_{Y^{\otimes n}} ( X^{\otimes n} ) ] + \log Y^{\otimes n} ) \}</script>

<p>By the first two statements in Theorem 1 and Lemma 1, one has</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\text{RHS} & = \frac{1}{n} \log \mathrm{Tr}\, ( \mathcal{P}_{Y^{\otimes n}} ( X^{\otimes n} ) Y^{\otimes n} ) + \frac{\log \mathsf{poly} (n)}{n} \notag \\
& = \frac{1}{n} \log \mathrm{Tr} ( X^{\otimes n} Y^{\otimes n} ) + \frac{\log \mathsf{poly} (n)}{n} \notag \\
& = \log \mathrm{Tr}\, ( X Y ) + \frac{\log \mathsf{poly} (n)}{n} . 
\end{align} %]]&gt;</script>

<p>Then one obtains the Golden-Thompson Inequality by letting $n \to \infty$.</p>
]]></content>
  </entry>
  
</feed>
